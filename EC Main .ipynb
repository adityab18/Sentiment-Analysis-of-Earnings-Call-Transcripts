{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "12e49528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_pdf_text(pdf_file):\n",
    "    all_text = \"\"\n",
    "    with open(pdf_file, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        for page_number in range(num_pages):\n",
    "            page = reader.pages[page_number]\n",
    "            page_text = page.extract_text()\n",
    "            all_text += page_text + \"\\n\"\n",
    "\n",
    "    return all_text\n",
    "\n",
    "#cleaning the text by removing the headers and footers\n",
    "def remove_date_name(text):\n",
    "    patterns = [\n",
    "        r'Page\\s+\\d+\\s+of\\s+\\d+', \n",
    "        r'Page \\d+ of \\d+\\n?',\n",
    "        r'Company Name\\s*\\n?(January|February|March|April|May|June|July|August|September|October|November|December)\\s*\\d{1,2}, \\d{4}\\n?',\n",
    "        r'Company Name Limited\\s*\\n?(January|February|March|April|May|June|July|August|September|October|November|December)\\s*\\d{1,2}, \\d{4}\\n?' \n",
    "    ]\n",
    "    \n",
    "    cleaned_text = text\n",
    "    for pattern in patterns:\n",
    "        cleaned_text = re.sub(pattern, '', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "#seperating sections as intro part and QnA part\n",
    "def separate_sections(text, pattern):\n",
    "    match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "    if match:\n",
    "        intro_part = text[:match.start()].strip()\n",
    "        qna_part = text[match.start():].strip()\n",
    "    else:\n",
    "        intro_part = text.strip()\n",
    "        qna_part = \"\"\n",
    "    \n",
    "    return intro_part, qna_part\n",
    "\n",
    "#identifying stopwords as per LM dictionary\n",
    "def preprocess_text(text):\n",
    "    with open(\"stopwords.txt\", \"r\") as f:\n",
    "        stopwords = f.read().split(\"\\n\")[:-1]  \n",
    "    words = text.split()\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    return \" \".join(words)\n",
    "\n",
    "#removing blank spaces\n",
    "def blank_spaces(text):\n",
    "    pattern = r' {3}\\n'\n",
    "    replacement = '  \\n'\n",
    "    modified_text = re.sub(pattern, replacement, text)\n",
    "    return modified_text\n",
    "\n",
    "#analysing the text and categorising as per LM dictionary\n",
    "def analyze_text(text, lm_dict):\n",
    "    pos_words = lm_dict[lm_dict[\"Positive\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    neg_words = lm_dict[lm_dict[\"Negative\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    uncern_words = lm_dict[lm_dict[\"Uncertainty\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    lit_words = lm_dict[lm_dict[\"Litigious\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    str_mdl__words = lm_dict[lm_dict[\"Strong_Modal\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    wk_mdl__words = lm_dict[lm_dict[\"Weak_Modal\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    cons_words = lm_dict[lm_dict[\"Constraining\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    comp_words = lm_dict[lm_dict[\"Complexity\"] != 0][\"Word\"].str.lower().to_list()\n",
    "\n",
    "\n",
    "    n = len(text.split())\n",
    "    n_pos = len([w for w in text.split() if w in pos_words])\n",
    "    n_neg = len([w for w in text.split() if w in neg_words])\n",
    "    n_uncern = len([w for w in text.split() if w in uncern_words])\n",
    "    n_lit = len([w for w in text.split() if w in lit_words])\n",
    "    n_str_modal = len([w for w in text.split() if w in str_mdl__words])\n",
    "    n_wk_modal = len([w for w in text.split() if w in wk_mdl__words])\n",
    "    n_cons = len([w for w in text.split() if w in cons_words])\n",
    "    n_comp = len([w for w in text.split() if w in comp_words])\n",
    "\n",
    "    results = {\n",
    "        \"Number of words\": n,\n",
    "        \"Uncertain words\": n_uncern,\n",
    "        \"Positive words\": n_pos,\n",
    "        \"Negative words\": n_neg,\n",
    "        \"Litigious words\": n_lit,\n",
    "        \"Strong Modal\": n_str_modal,\n",
    "        \"Weak Modal\": n_wk_modal,\n",
    "        \"Constraints words\": n_cons,\n",
    "        \"Complexity\": n_comp\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "#patterns of names in order to classify them as names of speakers\n",
    "def add_newline_before_names(text):\n",
    "    name_patterns = [\n",
    "        r'\\b[A-Z][a-z]+ [A-Z][a-z]+:',  \n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+ [A-Z]\\. [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z] [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]{2} [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.[A-Z]\\. [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\. [A-Z]\\. [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.[A-Z]\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\s[A-Z][a-z]+:',\n",
    "        r'\\bDr\\. [A-Z][a-z]+ [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z]\\.\\s[A-Z][a-z]+:'\n",
    "        r'\\b[A-Z]\\.[A-Z]\\. [A-Z][a-z]+ [A-Z][a-z]+:',\n",
    "        r'\\bDr\\. [A-Z][a-z]+ [A-Z]\\. [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+ [A-Z][a-z]+ [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]{2}+\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z]\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\. [A-Z][a-z]+ [A-Z][a-z]+:',\n",
    "        r'\\bDr\\. [A-Z][a-z]+ [A-Z]\\b [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\. [A-Z][a-z]+:',      \n",
    "        r'\\b[A-Z][a-z]+ [A-Z]+:',      \n",
    "        r'\\b[A-Z]\\. [A-Z]+:',          \n",
    "        r'\\b[A-Z][a-z]+:',\n",
    "        r'\\b[A-Za-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b([A-Z]\\.){3}\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b([A-Z]\\.\\s){3}[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\s([A-Z]\\.){3}+:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z]\\.\\s[A-Z]\\.:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\s[A-Z]\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\bDr\\.\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z]\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]{2}\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+-[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b([A-Z]\\.){2}[A-Z][a-z]+:',\n",
    "        r'\\b(Mr|Ms|Mrs|Dr|Prof)\\.\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b(Mr|Ms|Mrs|Dr|Prof)\\.\\s([A-Z]\\.\\s){2}[A-Z][a-z]+:',\n",
    "        r'\\b(Mr|Ms|Mrs|Dr|Prof)\\.\\s[A-Z]\\s[A-Z][a-z]+:',\n",
    "        r'\\b(Mr|Ms|Mrs|Dr|Prof)\\.\\s[A-Z]{2}\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z][â€™\\w]+:',\n",
    "        r'\\b([A-Z]\\s)+[A-Z][a-z]+:'\n",
    "    ]\n",
    "    \n",
    "    combined_pattern = r'|'.join(name_patterns)\n",
    "    regex = re.compile(combined_pattern)\n",
    "    matches = regex.finditer(text)\n",
    "    \n",
    "    segments = []\n",
    "    last_end = 0\n",
    "\n",
    "    for match in matches:\n",
    "        start, end = match.start(), match.end()\n",
    "\n",
    "        if start > 0 and text[start - 1] != '\\n':\n",
    "            segments.append(text[last_end:start] + '\\n' + text[start:end])\n",
    "        else:\n",
    "            segments.append(text[last_end:end])\n",
    "\n",
    "        last_end = end\n",
    "\n",
    "    segments.append(text[last_end:])\n",
    "    modified_text = ''.join(segments)\n",
    "\n",
    "    return modified_text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    pattern = r'(?<!\\n)\\n(?!([A-Z][a-z]*)(?: [A-Z][a-z]*){0,2} :| {3}[A-Z][a-z]*(?: [A-Z][a-z]*){0,2} :)'\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    return cleaned_text\n",
    "\n",
    "#seperating questions and answers from the QnA part based on the speakers\n",
    "def separate_questions_answers(text, management):\n",
    "    paragraphs = text.split('\\n')\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    management_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, management)) + r')\\b\\s*:')\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph = paragraph.strip()\n",
    "        \n",
    "        if management_pattern.match(paragraph):\n",
    "            questions.append(paragraph)\n",
    "        else:\n",
    "            answers.append(paragraph)\n",
    "\n",
    "    return \" \".join(answers), \" \".join(questions)\n",
    "\n",
    "#analysis\n",
    "def process_pdf_analysis(pdf_file, lm_dict_file):\n",
    "    lm_dict = pd.read_csv(lm_dict_file)\n",
    "    all_text = extract_pdf_text(pdf_file)\n",
    "    all_text = remove_date_name(all_text)\n",
    "    all_text = clean_text(all_text)\n",
    "    all_text = add_newline_before_names(all_text)\n",
    "    pattern = r\"\" #enter the line seperating the Introduction and QnA session\n",
    "    intro_part, qna_part = separate_sections(all_text, pattern)\n",
    "    \n",
    "    intro_part = preprocess_text(intro_part)\n",
    "    results_intro = analyze_text(intro_part, lm_dict)\n",
    "    \n",
    "    qna_part = blank_spaces(qna_part)\n",
    "    management =  [] #enter the names of the speakers of company's Management\n",
    "    questions_text, answers_text = separate_questions_answers(qna_part, management)\n",
    "    \n",
    "    questions_text = preprocess_text(questions_text)\n",
    "    answers_text = preprocess_text(answers_text)\n",
    "    \n",
    "    results_questions = analyze_text(questions_text, lm_dict)   \n",
    "    results_answers = analyze_text(answers_text, lm_dict)\n",
    "\n",
    "    return results_intro, results_questions, results_answers\n",
    "\n",
    "#saving the results\n",
    "def save_results_to_csv(results, csv_file, company, year):\n",
    "    index_tuples = [\n",
    "        (company, year, \"Introductory Part\"),\n",
    "        (company, year, \"Questions\"),\n",
    "        (company, year, \"Answers\")\n",
    "    ]\n",
    "    multi_index = pd.MultiIndex.from_tuples(index_tuples, names=[\"Company\", \"Year\", \"Section\"])\n",
    "    \n",
    "    df = pd.DataFrame(results, index=multi_index)\n",
    "    df.to_csv(csv_file, mode='a', header=not pd.io.common.file_exists(csv_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "b51996c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to final_sentiment_count.csv\n"
     ]
    }
   ],
   "source": [
    "pdf_file = r\"\" #enter the path of the earning transcript\n",
    "lm_dict_file = \"Loughran-McDonald_MasterDictionary.csv\" #load LM dictionary\n",
    "csv_file = \"final_sentiment_count.csv\" #save the results\n",
    "\n",
    "company = \"\" #enter the company name\n",
    "year = \"\" #enter the FY and financial quarter \n",
    "\n",
    "\n",
    "results_intro, results_questions, results_answers = process_pdf_analysis(pdf_file, lm_dict_file)\n",
    "results = [results_intro, results_questions, results_answers]\n",
    "save_results_to_csv(results, csv_file, company, year)\n",
    "\n",
    "print(f\"Results saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "e47e674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "doc = pd.read_csv('final_sentiment_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "id": "c08455c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Year</th>\n",
       "      <th>Section</th>\n",
       "      <th>Number of words</th>\n",
       "      <th>Uncertain words</th>\n",
       "      <th>Positive words</th>\n",
       "      <th>Negative words</th>\n",
       "      <th>Litigious words</th>\n",
       "      <th>Strong Modal</th>\n",
       "      <th>Weak Modal</th>\n",
       "      <th>Constraints words</th>\n",
       "      <th>Complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asian Paints</td>\n",
       "      <td>Q1 FY28</td>\n",
       "      <td>Introductory Part</td>\n",
       "      <td>469</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asian Paints</td>\n",
       "      <td>Q1 FY28</td>\n",
       "      <td>Questions</td>\n",
       "      <td>1624</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asian Paints</td>\n",
       "      <td>Q1 FY28</td>\n",
       "      <td>Answers</td>\n",
       "      <td>1614</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asian Paints</td>\n",
       "      <td>Q2 FY28</td>\n",
       "      <td>Introductory Part</td>\n",
       "      <td>538</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Asian Paints</td>\n",
       "      <td>Q2 FY28</td>\n",
       "      <td>Questions</td>\n",
       "      <td>1762</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>Wipro</td>\n",
       "      <td>Q2 FY24</td>\n",
       "      <td>Questions</td>\n",
       "      <td>796</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>Wipro</td>\n",
       "      <td>Q2 FY24</td>\n",
       "      <td>Answers</td>\n",
       "      <td>1409</td>\n",
       "      <td>14</td>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>Wipro</td>\n",
       "      <td>Q3 FY24</td>\n",
       "      <td>Introductory Part</td>\n",
       "      <td>1474</td>\n",
       "      <td>10</td>\n",
       "      <td>77</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>Wipro</td>\n",
       "      <td>Q3 FY24</td>\n",
       "      <td>Questions</td>\n",
       "      <td>539</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>Wipro</td>\n",
       "      <td>Q3 FY24</td>\n",
       "      <td>Answers</td>\n",
       "      <td>650</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2151 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Company     Year            Section  Number of words  \\\n",
       "0     Asian Paints  Q1 FY28  Introductory Part              469   \n",
       "1     Asian Paints  Q1 FY28          Questions             1624   \n",
       "2     Asian Paints  Q1 FY28            Answers             1614   \n",
       "3     Asian Paints  Q2 FY28  Introductory Part              538   \n",
       "4     Asian Paints  Q2 FY28          Questions             1762   \n",
       "...            ...      ...                ...              ...   \n",
       "2146         Wipro  Q2 FY24          Questions              796   \n",
       "2147         Wipro  Q2 FY24            Answers             1409   \n",
       "2148         Wipro  Q3 FY24  Introductory Part             1474   \n",
       "2149         Wipro  Q3 FY24          Questions              539   \n",
       "2150         Wipro  Q3 FY24            Answers              650   \n",
       "\n",
       "      Uncertain words  Positive words  Negative words  Litigious words  \\\n",
       "0                   4              10              10                1   \n",
       "1                  25              23              51                1   \n",
       "2                  32              30              37                1   \n",
       "3                   3              15              14                1   \n",
       "4                  25              22              46                5   \n",
       "...               ...             ...             ...              ...   \n",
       "2146               17              21              29                2   \n",
       "2147               14              36              23                1   \n",
       "2148               10              77              19                3   \n",
       "2149               10               8              22                0   \n",
       "2150                4              18              16                0   \n",
       "\n",
       "      Strong Modal  Weak Modal  Constraints words  Complexity  \n",
       "0                4           3                  1          10  \n",
       "1               17          23                  1           4  \n",
       "2               46          18                  7           2  \n",
       "3                3           3                  1           9  \n",
       "4               19          14                  2           0  \n",
       "...            ...         ...                ...         ...  \n",
       "2146             2          15                  0           0  \n",
       "2147            32           4                  1           2  \n",
       "2148            20           5                  0           9  \n",
       "2149             5           9                  0           1  \n",
       "2150            12           1                  0           0  \n",
       "\n",
       "[2151 rows x 12 columns]"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58268d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79dee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "412b1d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d62946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105274b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0e8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31702f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a59f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
