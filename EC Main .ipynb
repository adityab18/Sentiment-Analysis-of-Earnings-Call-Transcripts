{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "12e49528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "\n",
    "#extracting text from the pdf\n",
    "def extract_pdf_text(pdf_file):\n",
    "    all_text = \"\"\n",
    "    with open(pdf_file, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        for page_number in range(num_pages):\n",
    "            page = reader.pages[page_number]\n",
    "            page_text = page.extract_text()\n",
    "            all_text += page_text + \"\\n\"\n",
    "\n",
    "    return all_text\n",
    "\n",
    "#cleaning the text by removing the headers and footers\n",
    "def remove_date_name(text):\n",
    "    patterns = [\n",
    "        r'Page\\s+\\d+\\s+of\\s+\\d+', \n",
    "        r'Page \\d+ of \\d+\\n?',\n",
    "        r'Company Name\\s*\\n?(January|February|March|April|May|June|July|August|September|October|November|December)\\s*\\d{1,2}, \\d{4}\\n?',\n",
    "        r'Company Name Limited\\s*\\n?(January|February|March|April|May|June|July|August|September|October|November|December)\\s*\\d{1,2}, \\d{4}\\n?' \n",
    "    ]\n",
    "    \n",
    "    cleaned_text = text\n",
    "    for pattern in patterns:\n",
    "        cleaned_text = re.sub(pattern, '', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "#seperating sections as intro part and QnA part\n",
    "def separate_sections(text, pattern):\n",
    "    match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "    if match:\n",
    "        intro_part = text[:match.start()].strip()\n",
    "        qna_part = text[match.start():].strip()\n",
    "    else:\n",
    "        intro_part = text.strip()\n",
    "        qna_part = \"\"\n",
    "    \n",
    "    return intro_part, qna_part\n",
    "\n",
    "#identifying stopwords as per LM dictionary\n",
    "def preprocess_text(text):\n",
    "    with open(\"stopwords.txt\", \"r\") as f:\n",
    "        stopwords = f.read().split(\"\\n\")[:-1]  \n",
    "    words = text.split()\n",
    "    words = [w.lower() for w in words]\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    return \" \".join(words)\n",
    "\n",
    "#removing blank spaces\n",
    "def blank_spaces(text):\n",
    "    pattern = r' {3}\\n'\n",
    "    replacement = '  \\n'\n",
    "    modified_text = re.sub(pattern, replacement, text)\n",
    "    return modified_text\n",
    "\n",
    "#analysing the text and categorising as per LM dictionary\n",
    "def analyze_text(text, lm_dict):\n",
    "    pos_words = lm_dict[lm_dict[\"Positive\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    neg_words = lm_dict[lm_dict[\"Negative\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    uncern_words = lm_dict[lm_dict[\"Uncertainty\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    lit_words = lm_dict[lm_dict[\"Litigious\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    str_mdl__words = lm_dict[lm_dict[\"Strong_Modal\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    wk_mdl__words = lm_dict[lm_dict[\"Weak_Modal\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    cons_words = lm_dict[lm_dict[\"Constraining\"] != 0][\"Word\"].str.lower().to_list()\n",
    "    comp_words = lm_dict[lm_dict[\"Complexity\"] != 0][\"Word\"].str.lower().to_list()\n",
    "\n",
    "\n",
    "    n = len(text.split())\n",
    "    n_pos = len([w for w in text.split() if w in pos_words])\n",
    "    n_neg = len([w for w in text.split() if w in neg_words])\n",
    "    n_uncern = len([w for w in text.split() if w in uncern_words])\n",
    "    n_lit = len([w for w in text.split() if w in lit_words])\n",
    "    n_str_modal = len([w for w in text.split() if w in str_mdl__words])\n",
    "    n_wk_modal = len([w for w in text.split() if w in wk_mdl__words])\n",
    "    n_cons = len([w for w in text.split() if w in cons_words])\n",
    "    n_comp = len([w for w in text.split() if w in comp_words])\n",
    "\n",
    "    results = {\n",
    "        \"Number of words\": n,\n",
    "        \"Uncertain words\": n_uncern,\n",
    "        \"Positive words\": n_pos,\n",
    "        \"Negative words\": n_neg,\n",
    "        \"Litigious words\": n_lit,\n",
    "        \"Strong Modal\": n_str_modal,\n",
    "        \"Weak Modal\": n_wk_modal,\n",
    "        \"Constraints words\": n_cons,\n",
    "        \"Complexity\": n_comp\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "#patterns of names in order to classify them as names of speakers\n",
    "def add_newline_before_names(text):\n",
    "    name_patterns = [\n",
    "        r'\\b[A-Z][a-z]+ [A-Z][a-z]+:',  \n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+ [A-Z]\\. [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z] [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]{2} [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.[A-Z]\\. [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\. [A-Z]\\. [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.[A-Z]\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\s[A-Z][a-z]+:',\n",
    "        r'\\bDr\\. [A-Z][a-z]+ [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z]\\.\\s[A-Z][a-z]+:'\n",
    "        r'\\b[A-Z]\\.[A-Z]\\. [A-Z][a-z]+ [A-Z][a-z]+:',\n",
    "        r'\\bDr\\. [A-Z][a-z]+ [A-Z]\\. [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+ [A-Z][a-z]+ [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]{2}+\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z]\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\. [A-Z][a-z]+ [A-Z][a-z]+:',\n",
    "        r'\\bDr\\. [A-Z][a-z]+ [A-Z]\\b [A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\. [A-Z][a-z]+:',      \n",
    "        r'\\b[A-Z][a-z]+ [A-Z]+:',      \n",
    "        r'\\b[A-Z]\\. [A-Z]+:',          \n",
    "        r'\\b[A-Z][a-z]+:',\n",
    "        r'\\b[A-Za-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b([A-Z]\\.){3}\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b([A-Z]\\.\\s){3}[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\s([A-Z]\\.){3}+:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z]\\.\\s[A-Z]\\.:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\s[A-Z]\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\bDr\\.\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z]\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]{2}\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+-[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b([A-Z]\\.){2}[A-Z][a-z]+:',\n",
    "        r'\\b(Mr|Ms|Mrs|Dr|Prof)\\.\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b(Mr|Ms|Mrs|Dr|Prof)\\.\\s([A-Z]\\.\\s){2}[A-Z][a-z]+:',\n",
    "        r'\\b(Mr|Ms|Mrs|Dr|Prof)\\.\\s[A-Z]\\s[A-Z][a-z]+:',\n",
    "        r'\\b(Mr|Ms|Mrs|Dr|Prof)\\.\\s[A-Z]{2}\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.[A-Z]\\.\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z]\\.\\s[A-Z][a-z]+\\s[A-Z][a-z]+:',\n",
    "        r'\\b[A-Z][a-z]+\\s[A-Z][â€™\\w]+:',\n",
    "        r'\\b([A-Z]\\s)+[A-Z][a-z]+:'\n",
    "    ]\n",
    "    \n",
    "    combined_pattern = r'|'.join(name_patterns)\n",
    "    regex = re.compile(combined_pattern)\n",
    "    matches = regex.finditer(text)\n",
    "    \n",
    "    segments = []\n",
    "    last_end = 0\n",
    "\n",
    "    for match in matches:\n",
    "        start, end = match.start(), match.end()\n",
    "\n",
    "        if start > 0 and text[start - 1] != '\\n':\n",
    "            segments.append(text[last_end:start] + '\\n' + text[start:end])\n",
    "        else:\n",
    "            segments.append(text[last_end:end])\n",
    "\n",
    "        last_end = end\n",
    "\n",
    "    segments.append(text[last_end:])\n",
    "    modified_text = ''.join(segments)\n",
    "\n",
    "    return modified_text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    pattern = r'(?<!\\n)\\n(?!([A-Z][a-z]*)(?: [A-Z][a-z]*){0,2} :| {3}[A-Z][a-z]*(?: [A-Z][a-z]*){0,2} :)'\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    return cleaned_text\n",
    "\n",
    "#seperating questions and answers from the QnA part based on the speakers\n",
    "def separate_questions_answers(text, management):\n",
    "    paragraphs = text.split('\\n')\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    management_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, management)) + r')\\b\\s*:')\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph = paragraph.strip()\n",
    "        \n",
    "        if management_pattern.match(paragraph):\n",
    "            questions.append(paragraph)\n",
    "        else:\n",
    "            answers.append(paragraph)\n",
    "\n",
    "    return \" \".join(answers), \" \".join(questions)\n",
    "\n",
    "#analysis\n",
    "def process_pdf_analysis(pdf_file, lm_dict_file):\n",
    "    lm_dict = pd.read_csv(lm_dict_file)\n",
    "    all_text = extract_pdf_text(pdf_file)\n",
    "    all_text = remove_date_name(all_text)\n",
    "    all_text = clean_text(all_text)\n",
    "    all_text = add_newline_before_names(all_text)\n",
    "    pattern = r\"\" #enter the line seperating the Introduction and QnA session\n",
    "    intro_part, qna_part = separate_sections(all_text, pattern)\n",
    "    \n",
    "    intro_part = preprocess_text(intro_part)\n",
    "    results_intro = analyze_text(intro_part, lm_dict)\n",
    "    \n",
    "    qna_part = blank_spaces(qna_part)\n",
    "    management =  [] #enter the names of the speakers of company's Management\n",
    "    questions_text, answers_text = separate_questions_answers(qna_part, management)\n",
    "    \n",
    "    questions_text = preprocess_text(questions_text)\n",
    "    answers_text = preprocess_text(answers_text)\n",
    "    \n",
    "    results_questions = analyze_text(questions_text, lm_dict)   \n",
    "    results_answers = analyze_text(answers_text, lm_dict)\n",
    "\n",
    "    return results_intro, results_questions, results_answers\n",
    "\n",
    "#saving the results\n",
    "def save_results_to_csv(results, csv_file, company, year):\n",
    "    index_tuples = [\n",
    "        (company, year, \"Introductory Part\"),\n",
    "        (company, year, \"Questions\"),\n",
    "        (company, year, \"Answers\")\n",
    "    ]\n",
    "    multi_index = pd.MultiIndex.from_tuples(index_tuples, names=[\"Company\", \"Year\", \"Section\"])\n",
    "    \n",
    "    df = pd.DataFrame(results, index=multi_index)\n",
    "    df.to_csv(csv_file, mode='a', header=not pd.io.common.file_exists(csv_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "b51996c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to final_sentiment_count.csv\n"
     ]
    }
   ],
   "source": [
    "pdf_file = r\"\" #enter the path of the earning transcript\n",
    "lm_dict_file = \"Loughran-McDonald_MasterDictionary.csv\" #load LM dictionary\n",
    "csv_file = \"final_sentiment_count.csv\" #save the results\n",
    "\n",
    "company = \"\" #enter the company name\n",
    "year = \"\" #enter the FY and financial quarter \n",
    "\n",
    "\n",
    "results_intro, results_questions, results_answers = process_pdf_analysis(pdf_file, lm_dict_file)\n",
    "results = [results_intro, results_questions, results_answers]\n",
    "save_results_to_csv(results, csv_file, company, year)\n",
    "\n",
    "print(f\"Results saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e674e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08455c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58268d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79dee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "412b1d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d62946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105274b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0e8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31702f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a59f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
